---
title: "Modeling survival on the Titanic"
output:
  html_document:
    df_print: paged
---

```{r setup, message=FALSE}
library(tidyverse)
library(haven)
library(stringr)
library(visdat)
library(GGally)
library(mice)
library(broom)
```

## Playing the whole game

We're now going to put these tools together to import and clean a data set, explore and model the data, and present the results. Our end goal is to create a model that predicts survival in passengers who were on the Titanic. 

## Import and clean the data

The Titanic data is in the folder in a Stata file (.dta). Load the data, saving it as `titanic`.

```{r}
# import the data

```

Let's look at how many people survived. First, transform the `survived` and `pclass` to factors. Then, summarize how many people. You can use dplyr to group by survival and then summarize with `n()` or use the `table()` function on `titanic$survived`. 

```{r}
titanic <- titanic %>% 
  ______(survived = factor(survived, labels = c("Died", "Survived")),
         pclass = factor(pclass))

# fill in the code below or use table()
titanic %>% 
  ______(______) %>% 
  ______(n = n())
```

Take a look at the names of the columns in `titanic` with the `names()` function. 

```{r}
# look at the names

```

The `name` variable also contains information on the passenger's title. We're going to use a what are called regular expressions (a way of searching and manipulating strings) to find the title. You don't need to edit that part of the code; call the new variable `title`. We'll also clean up a few titles that mean the same thing.

```{r}
# pass the data set to `mutate()` and save the results.
______ ___ _______ ___
  #  take titles from name
  mutate(______ = str_replace_all(name, "(.*, )|(\\..*)", ""),
         # match abbreviations and French titles to miss and mrs
         title = case_when(title %in% c("Mlle", "Ms") ~ "Miss",
                      title == "Mme" ~ "Mrs",
                      TRUE ~ title),
         # make title a factor
         title = factor(title))
```

Look at how many titles there are. Like above, you can use dplyr or `table()`.

```{r}
# look at the count of titles with dplyr or table
```

There are a lot of unusual and uncommon titles. Since this is a categorical variable, let's collapse those rare ones into a single category called "Other". 

1. Mutate the `title` variable and use `fct_lump()` from the forcats package (already loaded) to only keep the top 4 titles. Look at the help page for this function if you don't know how. 

Make sure you save the results of `mutate()` to `titanic`.

2. Look at the counts for `title` now.

```{r}
# collapse uncommon titles into a single category with fct_lump()

# look at the counts for title again
 
```

## Missing data

How complete are the data? While we've learned a lot about the passengers on the Titanic since 1912, there are still some things we don't know. 

1. Use the `vis_miss()` function from the visdat package on `titanic` to visualize the missing data. 

What do you see?

```{r}
# visualize the missing data

```

R has many tools for working with missing data. One of the most popular tools for multiple imputation of missing data is the mice package. Let's impute for the `age` and `embarked` variables. We won't be including `body` (the body identification number) in the model, so we don't need to impute it.

The main functions for mice are `mice()`, which imputes the data, and `complete()`, which pools the results.

1. Use dplyr to send the following variables to `mice()`: age, pclass, sex, sibsp, parch, embarked, and title.
2. Save the imputed data as `imputed_titanic`. 

We'll save the imputed variables to the data as `mice_age` and `mice_embarked`, but you don't need to change that code.

```{r}
_______ <- titanic %>% 
  ______ ___
  mice(method = "norm.predict") %>% 
  complete() 

titanic$mice_age <- imputed_titanic$age
titanic$mice_embarked <- imputed_titanic$embarked
```

Does imputing age have a big impact on the estimate for its relationship with survival? Survival is a binary variable, so we need a logistic model. 

1. Fit the model for the effect of `age` on `survived` and tidy the results. 
2. Do the same for `mice_age`. 

Are they different?

```{r}

______(survived ~ age, data = titanic, family = binomial()) %>% 
  ______()

# Do the same for `mice_age`


```

It might also be useful to visualize their distributions. 

1. Subset the two age columns
2. Use tidyr to gather the variable names into one column and the values into another. Since you're only passing two columns, you don't need to change any function arguments. Remember the new columns are called "key" and "value" by default.
3. Color and fill the density plots by `key`.

```{r}
# 
titanic %>% 
  ______ ___ 
  ______() %>% 
  ggplot(aes(x = value, color = ______, fill = ______)) +
    geom_density(alpha = .7)
```

## Exploratory analysis

There are many ways to go about exploring your data, but let's use a simple one: paired plots. The `ggpairs()` function from the GGally package creates a matrix of plots for all the variables in your data set. `ggpairs()` supports all types of variables (numeric, categorical, etc) and will create the a plot appropriate for the two variables.

1. Subset the survived, mice_age, pclass, sex, and title columns.
2. Pass the results to `ggpairs()`. Color the results by `survived`.

It may take a little bit of time for `ggpairs()` to plot everything.

```{r}
______ ___
  ______ ___
  # ggpairs doesn't like that the variables are labelled.
  # remove them with haven::zap_labels()
  zap_labels() %>% 
  #  color by survival status.
  #  only show age, class, sex, and title.
  ggpairs(mapping = aes(color = _______), columns = 2:5, legend = 1) + 
    theme_minimal() +
    #  put the legend at the bottom
    #  make the column/row text bold
    theme(legend.position = "bottom",
          strip.text = element_text(face = "bold"))
```

Paired plots contain a lot of information, which is a pro and a con. What do you see? 

## Modeling

We'll use a a logistic regression to predict survival. Remember that `survival=1` means survival, so a higher odds ratio means greater odds of survival.

1. Fit a model with survived as the outcome and mice_age, sex, pclass, sibsp, parch, mice_embarked, and title as predictors. Save the model as `survived_model`
2. Tidy the results.
3. Pass the results to `mutate_if()`. This function takes a predicate function (one that produces a `TRUE`/`FALSE` answer) and applies a function to any variables that result in `TRUE`. Here, we're going to round all variables that are numeric to 2 digits.

```{r}
_______ ___ glm(_______, data = titanic, family = binomial)

survived_model %>% 
  _______() %>% 
  #  round all numeric variables to 2 digits
  _______(is.numeric, round, digits = 2)
```

What's going on with this model? Something looks wrong. Try to fix it.

1. Save the new model as `survived_model2`
2. Tidy the results and round the numeric variables.

```{r}
#  fit a new model


#  tidy the results


```

Did your changes work?

## Summarizing the results

Let's save a cleaned up table of the model results. We'll get the estimates and confidence intervals, exponentiation them, and round them. 

1. Tidy the results. Get the confidence intervals using the argument `conf.int = TRUE`.
2. Select only the term, estimate, and the upper and lower CIs. To get the confidence intervals, you can enter in the names directly or use dplyr helpers like `starts_with()`.
3. Exponentiate any variable that's numeric. The function for exponentiating is `exp()`.
4. Round any variable that's numeric.

```{r}
model_table <- survived_model2 %>% 
  ______ ___
  #  filter out the intercept. no need to change this line.
  filter(term != "(Intercept)") %>% 
  ______(term, estimate, _______) %>% 
  ______(______, exp) %>%
  mutate_if(is.numeric, ______, digits = ___)

model_table
```

How do the results look? Let's plot the odds ratios and CIs in a forest plot.

1. Arrange `model_table` by `estimate`.
2. Set `estimate` to the x-axis and `term` to the y-axis.
3. For the confidence intervals, we need to use `geom_errorbarh()`. We need to tell it where the lines begin and end. Use the variables `conf.low` and `conf.high` for `xmin` and `xmax`, respectively. 

```{r}
## Forest Plot
model_table %>% 
  ______ %>% 
  #  fct_inorder() helps keep the existing order othe data frame when we plot it
  mutate(term = fct_inorder(term)) %>% 
  ggplot(aes(x = ______, y = ______)) + 
    geom_point(size = 2) +
    geom_errorbarh(aes(xmin = ______, xmax = ______), height = 0) +
    geom_vline(xintercept = 1)
```

To know the real impact of a single variable on survival, we would need to thoughtfully create a model focused on getting an unbiased effect estimate. Since this is a prediction model, the individual odds ratios may or may not have a have a meaningful interpretation. 

Let's look at it from a different perspective: variable importance. In a linear model, variable importance is just the absolute value of the test statistic. The larger the value, the more important the variable to the model.

First, we need to re-tidy `survival_model2` because we got rid of `statistic`. Then, we'll create the variable importance variable and plot it.

1. Tidy the model
2. Sort the data frame using the absolute value of `statistic`. 
3. Create a new variable called `importance`
4. Plot `importance` on the x-axis and `term` on the y-axis.

```{r}
## Variable importance (abs(statistic))
_______ %>% 
  tidy() %>% 
  _______(abs(_______)) %>% 
  filter(term != "(Intercept)") %>% 
  mutate(importance = abs(_______),
         term = fct_inorder(term)) %>% 
  ggplot(aes(x = _______, y = _______)) + 
    geom_col() +
    coord_flip()
```

## Bonus: Machine Learning

R has excellent support for many other approaches to modeling. Two such packages are `rms`, focused on regression, and `caret`, focused on machine learning. If you're interested in machine learning, give this optional section a try.

Install the packages if you don't have them. Uncomment the code to be able to run it.

```{r}
# install.packages(c("caret", "rms", "randomForest", "gbm))
library(caret)
library(rms)
```

You can save formulas directly so that you don't need to keep re-entering them. We'll save the above formula as `fmla`. We'll also use a formula for a non-linear model that uses `rcs()` to fit a restricted cubic spline on `age` and quadratic terms for the `sibsp` and `parch` variables. These are count variables, which do better with quadratics than with spline terms.

```{r}
fmla <- survived ~ mice_age + pclass + sibsp + parch + mice_embarked + title
fmla_spline <- survived ~ rcs(mice_age) + pclass + sibsp + I(sibsp^2) + parch + I(parch^2) + mice_embarked + title
```

We'll also create a function so we don't have to keep entering arguments. We'll cover functions in more depth tomorrow. This one wraps around the `train()` function from the caret package, which fits many types of models. Our custom function is called `ml()`. You can use it to set the formula or method, as well as pass any other arguments to `train` that you want. 

`ml()` will fit a model with the specified model and get model metrics using repeated cross-validation. It will also automatically tune the more complicated machine learning algorithms.

```{r}
#  function to avoid setting defaults repeatedly
ml <- function(.fmla = fmla, method, ...) {
  train(.fmla,
        data = titanic,
        method = method, 
        ...,
        na.action  = na.omit,   
        metric = "ROC",
        trControl = trainControl(method = "repeatedcv",
           repeats = 5,	
           summaryFunction = twoClassSummary,	
           classProbs = TRUE))
}
```

We're going to fit models using four approaches: the same logistic regression as above, a logistic regression with the non-linear model, a random forest model, and a gradient boosted model. Run each and check the results. How do the ROC values look? This is only one way to measure model performance, but the closer the value to 1, the better the predictive ability.

How about the sensitivity and specificity? What's the trade off?

```{r}
logistic <- ml(method = "glm", family = binomial)
logistic
```

```{r}
logistic_spline <- ml(fmla_spline, method = "glm", family = binomial)
logistic_spline
```

```{r}
random_forest <- ml(fmla_spline, method = "rf", family = binomial)
random_forest
```

```{r}
boosted <- ml(fmla_spline, method = "gbm")
boosted
```

The caret package also has a built-in function for variable importance, `varImp()`. It returns a data frame. Run the code below to sort it. Which is the most important variable? Is the highest ranking variable different than from the logistic model?

```{r}
boosted$finalModel %>% 
  varImp() %>% 
  rownames_to_column("variable") %>% 
  arrange(desc(Overall))
```